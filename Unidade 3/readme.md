# ğŸ’»  OtimizaÃ§Ã£o AvanÃ§ada em Deep Learning com PyTorch: Uma AnÃ¡lise aplicada ao Fashion-MNIST

Este repositÃ³rio contÃ©m o cÃ³digo, experimentos e visualizaÃ§Ãµes desenvolvidas para a **Nota TÃ©cnica Aprofundada** do CapÃ­tulo 6 do livro *Deep Learning with PyTorch Step-by-Step*. O projeto aplica os conceitos de otimizaÃ§Ã£o de redes neurais ao modelo CNN LeNet-like treinado no popular dataset **Fashion-MNIST**.

---

## ğŸ‘©â€ğŸ’» Autores
* **Julia Alanne Silvino dos Santos**
* **Pablo Durkheim Fernandes do Nascimento**

Este trabalho foi desenvolvido como projeto final da disciplina de **PROJETO DE SISTEMAS BASEADOS EM APRENDIZADO DE MÃQUINA**.

## ğŸ“„ Nota TÃ©cnica e Objetivos

VocÃª pode conferir a anÃ¡lise detalhada, resultados e conclusÃµes na nota tÃ©cnica publicada:

> **ğŸ”— Link para o Artigo no Medium:**
> [`OtimizaÃ§Ã£o AvanÃ§ada em Deep Learning com PyTorch`](https://medium.com/@juliaalanne/otimiza%C3%A7%C3%A3o-avan%C3%A7ada-em-deep-learning-com-pytorch-uma-an%C3%A1lisem-aplicada-ao-fashion-mnist-8a0a7aa1095f?postPublishedType=repub)

### ğŸ¯ Objetivos TÃ©cnicos
O projeto tem como objetivo:

* Demonstrar a funÃ§Ã£o da **EWMA (Exponentially Weighted Moving Average)** no suavizamento de gradientes e o papel da **CorreÃ§Ã£o de ViÃ©s (Bias Correction)**.
* Comparar o desempenho e a trajetÃ³ria do **SGD Simples**, **SGD com Momentum** e **SGD com Nesterov**.
* Visualizar os **Gradientes Adaptados do Adam** (Primeiro e Segundo Momento) em um grÃ¡fico de trÃªs painÃ©is.
* Implementar e comparar diferentes **Learning Rate Schedulers** ($\text{StepLR}$ e $\text{LambdaLR}$) e analisar seu impacto em treinamentos curtos.

---
## ğŸ“¦ Estrutura do RepositÃ³rio



Com certeza! Aqui estÃ¡ o README formatado em Markdown, com emojis e uma estrutura clara.

Markdown

# ğŸ’» README do RepositÃ³rio GitHub: OtimizaÃ§Ã£o AvanÃ§ada em Deep Learning

Este repositÃ³rio contÃ©m o cÃ³digo, experimentos e visualizaÃ§Ãµes desenvolvidas para a **Nota TÃ©cnica Aprofundada** do CapÃ­tulo 6 do livro *Deep Learning with PyTorch Step-by-Step*. O projeto aplica os conceitos de otimizaÃ§Ã£o de redes neurais ao modelo CNN LeNet-like treinado no popular dataset **Fashion-MNIST**.

---

## ğŸ‘©â€ğŸ’» Autores
* **Julia Alanne Silvino dos Santos**
* **Pablo Durkheim Fernandes do Nascimento**

Este trabalho foi desenvolvido como projeto final da disciplina de **PROJETO DE SISTEMAS BASEADOS EM APRENDIZADO DE MÃQUINA**.

## ğŸ“„ Nota TÃ©cnica e Objetivos

VocÃª pode conferir a anÃ¡lise detalhada, resultados e conclusÃµes na nota tÃ©cnica publicada:

> **ğŸ”— Link para o Artigo no Medium/Substack:**
> `https://github.com/JuliaAlanne/MLOps/tree/main/Unidade%203`

### ğŸ¯ Objetivos TÃ©cnicos
O projeto cumpre os seguintes requisitos, adaptando-os para o **Fashion-MNIST**:

* Demonstrar a funÃ§Ã£o da **EWMA (Exponentially Weighted Moving Average)** no suavizamento de gradientes e o papel da **CorreÃ§Ã£o de ViÃ©s (Bias Correction)**.
* Comparar o desempenho e a trajetÃ³ria do **SGD Simples**, **SGD com Momentum** e **SGD com Nesterov**.
* Visualizar os **Gradientes Adaptados do Adam** (Primeiro e Segundo Momento) em um grÃ¡fico de trÃªs painÃ©is.
* Implementar e comparar diferentes **Learning Rate Schedulers** ($\text{StepLR}$ e $\text{LambdaLR}$) e analisar seu impacto em treinamentos curtos.

---

## ğŸ“¦ Estrutura do RepositÃ³rio


â”œâ”€â”€ notebooks/ # Notebooks contendo o cÃ³digo completo dos experimentos â”‚ â”œâ”€â”€ 01_EWMA_&_Adam.ipynb # EWMA, Bias Correction e VisualizaÃ§Ã£o Adam (3 painÃ©is) â”‚ â”œâ”€â”€ 02_SGD_Variants.ipynb # SGD, Momentum, Nesterov e comparaÃ§Ã£o de Loss/Accuracy â”‚ â””â”€â”€ 03_LR_Schedulers.ipynb # LR Range Test, StepLR, LambdaLR e anÃ¡lise de desempenho â”œâ”€â”€ figures/ # Pasta obrigatÃ³ria para armazenar todas as imagens geradas (para a NT) â””â”€â”€ src/ # MÃ³dulos Python e classe de treinamento â””â”€â”€ architecture.py # Classe de treinamento (StepByStep) adaptada para: # - Captura de Gradientes e ParÃ¢metros. # - LÃ³gica de Schedulers por Ã©poca e mini-batch.


---

## âš™ï¸ Como Executar o Projeto

Siga estas instruÃ§Ãµes para configurar o ambiente e reproduzir todos os experimentos.

### PrÃ©-requisitos
VocÃª precisarÃ¡ do Python 3.8+ e das seguintes bibliotecas:
* `torch`, `torchvision` (PyTorch)
* `numpy`, `matplotlib`
* `jupyterlab`

### 1. Clonar o RepositÃ³rio

```bash
git clone [https://github.com/JuliaAlanne/MLOps/tree/main/Unidade%203](https://github.com/JuliaAlanne/MLOps/tree/main/Unidade%203)
cd Unidade_3
